{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Fine-tuning Mistral-7B for Email Response Generation\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Mistral-7B model to generate professional email responses. We'll use a simple approach with LoRA (Low-Rank Adaptation) to make the training efficient and cost-effective.\n",
        "\n",
        "## Overview\n",
        "- **Model**: Mistral-7B-Instruct-v0.1\n",
        "- **Task**: Email response generation\n",
        "- **Method**: LoRA fine-tuning\n",
        "- **Dataset**: Custom email response dataset\n",
        "\n",
        "## Prerequisites\n",
        "- Hugging Face account with access token\n",
        "- GPU with at least 16GB VRAM (recommended)\n",
        "- Required Python packages (installed in next cell)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# Run this cell first to install all necessary dependencies\n",
        "\n",
        "!pip install -q transformers==4.36.0\n",
        "!pip install -q peft==0.6.2\n",
        "!pip install -q accelerate==0.25.0\n",
        "!pip install -q bitsandbytes==0.41.3\n",
        "!pip install -q datasets==2.14.6\n",
        "!pip install -q trl==0.7.4\n",
        "!pip install -q torch==2.1.1\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üîß Using device: {device}\")\n",
        "print(f\"üîß CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîß GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üîß GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Hugging Face authentication\n",
        "# Replace 'your_token_here' with your actual Hugging Face token\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Set your Hugging Face token here\n",
        "HF_TOKEN = \"your_token_here\"  # Replace with your actual token\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=HF_TOKEN)\n",
        "print(\"‚úÖ Successfully logged in to Hugging Face!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the training data\n",
        "# Load the demo email dataset from JSON file\n",
        "\n",
        "def load_email_data(file_path=\"email_demo_data.json\"):\n",
        "    \"\"\"\n",
        "    Load email training data from JSON file\n",
        "    \n",
        "    Returns:\n",
        "        list: List of email training examples\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {file_path} not found!\")\n",
        "        return []\n",
        "\n",
        "# Load the data\n",
        "email_data = load_email_data()\n",
        "print(f\"üìß Loaded {len(email_data)} email examples\")\n",
        "\n",
        "# Display first example\n",
        "if email_data:\n",
        "    print(\"\\nüîç First training example:\")\n",
        "    print(f\"Prompt: {email_data[0]['prompt']}\")\n",
        "    print(f\"Input: {email_data[0]['input'][:100]}...\")\n",
        "    print(f\"Response: {email_data[0]['response'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format data for training\n",
        "# Create training prompts in the format expected by Mistral\n",
        "\n",
        "def format_training_data(data):\n",
        "    \"\"\"\n",
        "    Format the email data for training with proper prompt structure\n",
        "    \n",
        "    Args:\n",
        "        data (list): List of email examples\n",
        "        \n",
        "    Returns:\n",
        "        list: Formatted training examples\n",
        "    \"\"\"\n",
        "    formatted_data = []\n",
        "    \n",
        "    for example in data:\n",
        "        # Create a conversation format prompt\n",
        "        prompt = f\"\"\"<s>[INST] {example['prompt']}\n",
        "\n",
        "Email context: {example['input']} [/INST]\n",
        "\n",
        "{example['response']}</s>\"\"\"\n",
        "        \n",
        "        formatted_data.append({\"text\": prompt})\n",
        "    \n",
        "    return formatted_data\n",
        "\n",
        "# Format the data\n",
        "formatted_data = format_training_data(email_data)\n",
        "print(f\"üìù Formatted {len(formatted_data)} training examples\")\n",
        "\n",
        "# Show formatted example\n",
        "if formatted_data:\n",
        "    print(\"\\nüîç Formatted training example:\")\n",
        "    print(formatted_data[0][\"text\"][:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the base Mistral model and tokenizer\n",
        "# We'll use 4-bit quantization to reduce memory usage\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Configure 4-bit quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "print(\"üîÑ Loading Mistral-7B model and tokenizer...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer loaded successfully!\")\n",
        "print(f\"üîß Model device: {model.device}\")\n",
        "print(f\"üîß Model dtype: {model.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for LoRA fine-tuning\n",
        "# LoRA allows us to fine-tune efficiently by only updating a small number of parameters\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                   # Rank of adaptation\n",
        "    lora_alpha=32,          # LoRA scaling parameter\n",
        "    target_modules=[        # Modules to apply LoRA to\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.1,       # LoRA dropout\n",
        "    bias=\"none\",            # Bias type\n",
        "    task_type=\"CAUSAL_LM\"   # Task type\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"Print the number of trainable parameters in the model\"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"üéØ Trainable params: {trainable_params:,}\")\n",
        "    print(f\"üéØ All params: {all_param:,}\")\n",
        "    print(f\"üéØ Trainable %: {100 * trainable_params / all_param:.2f}%\")\n",
        "\n",
        "print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training dataset\n",
        "# Convert our formatted data to a Hugging Face Dataset\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = Dataset.from_list(formatted_data)\n",
        "print(f\"üìä Training dataset size: {len(train_dataset)}\")\n",
        "\n",
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-email-finetuned\",       # Output directory\n",
        "    num_train_epochs=3,                           # Number of training epochs\n",
        "    per_device_train_batch_size=1,                # Batch size per device\n",
        "    gradient_accumulation_steps=4,                # Gradient accumulation steps\n",
        "    optim=\"adamw_torch\",                          # Optimizer\n",
        "    save_steps=50,                                # Save checkpoint every N steps\n",
        "    logging_steps=10,                             # Log every N steps\n",
        "    learning_rate=2e-4,                           # Learning rate\n",
        "    weight_decay=0.001,                           # Weight decay\n",
        "    fp16=True,                                    # Use mixed precision\n",
        "    max_grad_norm=0.3,                            # Max gradient norm\n",
        "    max_steps=-1,                                 # Max steps (-1 means no limit)\n",
        "    warmup_ratio=0.03,                            # Warmup ratio\n",
        "    group_by_length=True,                         # Group sequences by length\n",
        "    lr_scheduler_type=\"cosine\",                   # Learning rate scheduler\n",
        "    report_to=\"none\"                              # Don't report to wandb/tensorboard\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments configured!\")\n",
        "print(f\"üîß Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"üîß Total training steps: {len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the trainer\n",
        "# Use SFTTrainer for supervised fine-tuning\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,          # Maximum sequence length\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    packing=False,               # Don't pack multiple examples together\n",
        ")\n",
        "\n",
        "print(\"‚úÖ SFTTrainer initialized successfully!\")\n",
        "print(\"üöÄ Ready to start training...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "# This will take some time depending on your hardware\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"‚è∞ This may take 10-30 minutes depending on your GPU\")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training completed!\")\n",
        "print(\"üíæ Saving the fine-tuned model...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "# Save both the model and tokenizer for later use\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.model.save_pretrained(\"./mistral-email-finetuned-final\")\n",
        "tokenizer.save_pretrained(\"./mistral-email-finetuned-final\")\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer saved successfully!\")\n",
        "print(\"üìÅ Saved to: ./mistral-email-finetuned-final/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Testing the Fine-tuned Model\n",
        "\n",
        "Now let's test our fine-tuned model with some example email scenarios to see how well it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "# Create a text generation pipeline for easy inference\n",
        "\n",
        "def generate_email_response(prompt, email_context, max_length=300):\n",
        "    \"\"\"\n",
        "    Generate an email response using the fine-tuned model\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): The task description\n",
        "        email_context (str): The email context/input\n",
        "        max_length (int): Maximum length of generated response\n",
        "        \n",
        "    Returns:\n",
        "        str: Generated email response\n",
        "    \"\"\"\n",
        "    # Format the input like our training data\n",
        "    input_text = f\"<s>[INST] {prompt}\\n\\nEmail context: {email_context} [/INST]\\n\\n\"\n",
        "    \n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode the response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the generated part (after [/INST])\n",
        "    response_start = full_response.find(\"[/INST]\") + len(\"[/INST]\")\n",
        "    generated_response = full_response[response_start:].strip()\n",
        "    \n",
        "    return generated_response\n",
        "\n",
        "print(\"‚úÖ Email generation function ready!\")\n",
        "print(\"üß™ Ready to test the model...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with example scenarios\n",
        "# Let's test our model with some new email scenarios\n",
        "\n",
        "test_cases = [\n",
        "    {\n",
        "        \"prompt\": \"Write a professional email responding to a client's complaint about delayed delivery.\",\n",
        "        \"context\": \"Dear Support Team, I ordered my products 3 weeks ago and still haven't received them. Order #12345. This is very frustrating as I needed them for an important event. Please help! - Sarah Johnson\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write an email accepting a meeting invitation and suggesting agenda items.\",\n",
        "        \"context\": \"Hi Team, I'd like to schedule a project review meeting for next Wednesday at 2 PM. Please confirm your availability. We need to discuss current progress and next steps. - Project Manager\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a polite email declining a job offer due to salary expectations.\",\n",
        "        \"context\": \"Dear John, Thank you for offering me the Software Developer position at XYZ Corp. The role seems exciting, but the offered salary of $60k is below my expectations of $75k. Looking forward to your response. - HR Manager\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Test each scenario\n",
        "for i, test_case in enumerate(test_cases, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üß™ TEST CASE {i}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Prompt: {test_case['prompt']}\")\n",
        "    print(f\"Context: {test_case['context'][:100]}...\")\n",
        "    print(f\"\\nüìß Generated Response:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    response = generate_email_response(test_case['prompt'], test_case['context'])\n",
        "    print(response)\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "üéâ **Congratulations!** You have successfully fine-tuned Mistral-7B for email response generation.\n",
        "\n",
        "### What we accomplished:\n",
        "- ‚úÖ Loaded and prepared email training data\n",
        "- ‚úÖ Configured Mistral-7B with 4-bit quantization for efficiency\n",
        "- ‚úÖ Applied LoRA for parameter-efficient fine-tuning  \n",
        "- ‚úÖ Trained the model on email response examples\n",
        "- ‚úÖ Saved the fine-tuned model for future use\n",
        "- ‚úÖ Tested the model with new email scenarios\n",
        "\n",
        "### Model Performance:\n",
        "The fine-tuned model should now be able to generate professional email responses based on different contexts and prompts. The quality will improve with:\n",
        "- More training data\n",
        "- Longer training (more epochs)\n",
        "- Fine-tuning hyperparameters\n",
        "\n",
        "### Next Steps:\n",
        "1. **Expand the dataset**: Add more diverse email examples\n",
        "2. **Improve prompts**: Experiment with different prompt formats\n",
        "3. **Deploy the model**: Use the model in a web application or API\n",
        "4. **Evaluate performance**: Create metrics to measure email quality\n",
        "5. **Continue training**: Fine-tune further with domain-specific emails\n",
        "\n",
        "### Using the saved model:\n",
        "```python\n",
        "# Load the fine-tuned model later\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "model = PeftModel.from_pretrained(base_model, \"./mistral-email-finetuned-final\")\n",
        "```\n",
        "\n",
        "**Happy emailing! üìß‚ú®**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
